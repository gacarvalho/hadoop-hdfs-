![Sobre: Background](https://github.com/gacarvalho/hadoop-hdfs-project/blob/main/Image/background-hadoop-atualizado.png)


## üìå PROPOSTA DO PROJETO

O objetivo do projeto √© apresentar uma solu√ß√£o de dados utilizando o ecossistema Hadoop. O foco do projeto era manipular o HDFS, formatando o sistema de arquivos e executando opera√ß√µes com arquivos e diret√≥rios, aplicando uma codifica√ß√£o MapperReducer com Java. O objetivo final do projeto √© apresentar quais foram os jogos mais vendidos em cada ano, distribuindo o arquivo pelo HDFS. 


üì¢  ETAPA 1 : AN√ÅLISE DO BANCO DE DADOS RELACIONAL

A base de dados se encontra no SGBD MySQL, que √© composto por uma base de dados sobre jogos, plataforma, anoLancamento, genero, fabricante, na_sales (vendas na america), eu_sales (vendas na europa), jp_sales (vendas no jap√£o), other_sales, global_sales. 

![Sobre: Imagem Hadoop 1](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/1.png)
![Sobre: Imagem Hadoop 2](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/2.png)


üì¢  ETAPA 2 : FORMATANDO O NAMENODE

![Sobre: Imagem Hadoop 2](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/3.png)


üì¢  ETAPA 3 : EXTRA√á√ÉO DO MYSQL COM O SQOOP

Ap√≥s an√°lise da base de dados, foi necess√°rio extrair os dados do MySQL e alocar os dados no HDFS.

```bash
:/usr/local/sqoop$ bin/sqoop import --connect jdbc:mysql://localhost/IGTI?zeroDateTimeBehavior=convert_To_Null --username <usuario> --password <password> --table DADOS_GAME -m 1 --bindir /usr/local/sqoop/lib --target-dir /user/igti/DADOS_GAME
```
![Sobre: Imagem Hadoop 5](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/5.png)

Ap√≥s o alocamento da base de dados, √© poss√≠vel analisar o bloco da base de dados no pelo comando

```bash
:/usr/local/hadoop$ bin/hdfs dfs -ls /user/igti/DADOS_GAME/part-m-00000
```
![Sobre: Imagem Hadoop](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/7.png)

üì¢  ETAPA 4 : PROGRAMANDO O MAP REDUCE

Logo ap√≥s a an√°lise dos dados e extra√ß√£o do mesmo para o HDFS, foi necess√°rio realizar uma programa√ß√£o para aplicar o Mapper e e o Reducer, para isso, a linguagem escolhida foi Java. Na figura abaixo √© poss√≠vel observar a declara√ß√£o do JobConf e referenciando as classes Map e Reduce.

![Sobre: Imagem Hadoop 9](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/9.png)

Na imagem a seguir, j√° poss√≠vel analisar a programa√ß√£o da classe MapIGTI.

![Sobre: Imagem Hadoop 10](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/10.png)

Ap√≥s a programa√ß√£o da classe MapIGTI, a figura abaixo demonstra a programa√ß√£o da classe ReduceIGTI.

![Sobre: Imagem Hadoop 11](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/1.png)


üì¢  ETAPA 5 : CONSULTANDO O ARQUIVO FINALIZADO NO HDFS

Ap√≥s o processo de importa√ß√£o com o sqoop, tratamento pelo Mapper e Reducer, √© poss√≠vel analisar o arquivo final no HDFS no caminho:

```bash
:/usr/local/hadoop$ bin/hdfs dfs -cat /user/igti/PastaSaida/part-00000
```
![Sobre: Imagem Hadoop 14](https://github.com/gacarvalho/hadoop-hdfs-/blob/main/Image/14.png)

Na imagem acima, √© poss√≠vel observar que o objetivo do projeto foi concluido com sucesso, pois foi listado o jogo que MAIS FOI VENDIDO de acordo com cada ANO.  
